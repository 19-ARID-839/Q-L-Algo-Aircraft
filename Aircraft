import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
class AircraftEnv:
    def __init__(self, speed=1.0, goal=(10, 10), threshold=1.0, timeout_steps=50):
        self.speed = speed
        self.goal = np.array(goal)
        self.threshold = threshold
        self.position = np.array([0.0, 0.0])
        self.heading = 0.0  # Heading angle in radians
        self.path = [self.position.copy()]
        self.best_path = []  # Store the best path with least steps
        self.best_episode_info = None  # Store the episode details for the best path
        self.state_space_size = (20, 20, 36)  # (x, y, heading)
        self.action_space_size = 6  # 0 = Left, 1 = Right, 2 = Straight, 3 = Turn back, 4 = Down, 5 = Up
        self.q_table = np.zeros(self.state_space_size + (self.action_space_size,))
        self.timeout_steps = timeout_steps  # Max steps without progress before timeout
        self.obstacles = None  # Obstacles will be generated on each reset instead of at initialization
        self.total_reached = 0  # Count of episodes reaching the goal
        self.total_failed = 0  # Count of episodes failing to reach the goal

    def generate_obstacles(self, num_obstacles):
        obstacles = []
        for _ in range(num_obstacles):
            obstacle = np.random.uniform(-10, 10, size=2)
            obstacles.append(obstacle)
        return np.array(obstacles)

    def reset(self):
        self.position = np.array([0.0, 0.0])
        self.heading = 0.0
        self.path = [self.position.copy()]
        self.obstacles = self.generate_obstacles(6)  # 6 obstacles, placed randomly
        self.timeout_steps = 50  # Reset timeout_steps
        return self.get_state()

    def shaping_for_obstacles(self):
        penalty = 0.0
        for obs in self.obstacles:
            distance_to_obstacle = np.linalg.norm(self.position - obs)
            if distance_to_obstacle < 2.0:
                penalty -= (0.05 * (2 - distance_to_obstacle)**2)  # Smoother penalty curve
        return penalty

    def step(self, action):
        if action == 0:
            self.heading -= np.pi / 18  # Turn left by 10 degrees
        elif action == 1:
            self.heading += np.pi / 18  # Turn right by 10 degrees
        elif action == 2:
            # Move straight (forward)
            self.position += self.speed * np.array([np.cos(self.heading), np.sin(self.heading)])
        elif action == 3:
            # Turn back (180 degrees)
            self.heading += np.pi  # 180 degrees turn
        elif action == 4:
            # Move down (decrease y)
            self.position -= self.speed * np.array([0, 1])
        elif action == 5:
            # Move up (increase y)
            self.position += self.speed * np.array([0, 1])

        self.path.append(self.position.copy())  # Track the new position

        distance_to_goal = np.linalg.norm(self.position - self.goal)
        reward = 0

        # Reward for moving closer to the goal
        previous_distance = np.linalg.norm(self.path[-2] - self.goal) if len(self.path) > 1 else float('inf')
        reward += 5 * (previous_distance - distance_to_goal)

        # If the agent is closer to the goal, give a small positive reward
        if previous_distance > distance_to_goal:
            reward += 0.1  # Small positive reward for making progress towards the goal

        if self.is_goal_reached():
            reward += 100  # Large reward for goal
        elif self.is_collision():
            reward -= 50  # Penalty for hitting an obstacle
        else:
            reward += self.shaping_for_obstacles()

        return self.get_state(), reward, self.is_goal_reached()

    def is_goal_reached(self):
        return np.linalg.norm(self.position - self.goal) <= self.threshold

    def is_collision(self):
        return any(np.linalg.norm(self.position - obs) < 1.0 for obs in self.obstacles)

    def get_state(self):
        x, y = self.position
        heading_bin = int((self.heading % (2 * np.pi)) / (2 * np.pi / self.state_space_size[2]))
        heading_bin = np.clip(heading_bin, 0, self.state_space_size[2] - 1)
        x_bin = int(np.clip((x + 10) // 1, 0, self.state_space_size[0] - 1))
        y_bin = int(np.clip((y + 10) // 1, 0, self.state_space_size[1] - 1))
        return x_bin, y_bin, heading_bin

    def epsilon_greedy_policy(self, state, epsilon):
        if np.random.rand() < epsilon:
            return np.random.randint(self.action_space_size)  # Explore
        return np.argmax(self.q_table[state])  # Exploit

    def update_q_table(self, state, action, reward, next_state, alpha=0.1, gamma=0.99):
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + gamma * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        self.q_table[state][action] += alpha * td_error

    def train(self, num_episodes=2000, max_steps=200, epsilon_decay=0.995, min_epsilon=0.05):
        epsilon = 1.0
        best_steps = float('inf')
        best_path = None
        best_episode_info = None
        rewards_per_episode = []
        steps_per_episode = []  # Track steps per episode
        distances_per_episode = []  # Track total distance traveled per episode

        for episode in range(num_episodes):
            state = self.reset()
            goal_reached = False
            steps = 0
            total_distance = 0
            total_reward = 0
            previous_position = self.position.copy()
            timeout_steps = 50  # Reset timeout_steps at the beginning of each episode

            while not goal_reached and steps < max_steps:
                action = self.epsilon_greedy_policy(state, epsilon)
                next_state, reward, goal_reached = self.step(action)
                self.update_q_table(state, action, reward, next_state)
                state = next_state
                total_reward += reward
                total_distance += np.linalg.norm(self.position - self.goal)
                steps += 1

                # Timeout check
                if np.linalg.norm(self.position - previous_position) < 0.1:
                    timeout_steps -= 1
                    if timeout_steps <= 0:
                        goal_reached = True
                else:
                    timeout_steps = 50

                previous_position = self.position.copy()

            # Track episode results
            if goal_reached:
                self.total_reached += 1
            else:
                self.total_failed += 1

            avg_distance = total_distance / steps if steps > 0 else float('inf')
            rewards_per_episode.append(total_reward)
            steps_per_episode.append(steps)
            distances_per_episode.append(total_distance)

            # Update best path based on fewer steps
            if goal_reached and steps < best_steps:
                best_steps = steps
                best_path = self.path.copy()
                best_episode_info = (episode, goal_reached, avg_distance, steps)

            epsilon = max(min_epsilon, epsilon * epsilon_decay)

            # Visualize animation for each episode (every 100th episode)
            if episode % 100 == 0:  # Visualize every 100th episode to avoid overloading
                print(f"Animating Episode {episode}...")
                self.animate(path=self.path, episode_info=(episode, goal_reached, avg_distance, steps), show_details=True)

        self.best_path = best_path
        self.best_episode_info = best_episode_info

        return rewards_per_episode, steps_per_episode, distances_per_episode

    def animate(self, path=None, pause=0.1, title="Aircraft Simulation", show_details=False, episode_info=None):
        if path is None:
            path = self.best_path

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.set_xlim(-20, 20)
        ax.set_ylim(-20, 20)
        ax.axhline(0, color='black', linewidth=0.5)
        ax.axvline(0, color='black', linewidth=0.5)

        # Plot goal and start
        ax.scatter(self.goal[0], self.goal[1], color='yellow', s=200, label='Goal')
        ax.scatter(self.path[0][0], self.path[0][1], color='green', label='Start')

        # Obstacles with a single label
        ax.scatter(self.obstacles[:, 0], self.obstacles[:, 1], color='red', s=100, label='Obstacles')

        # Best path
        path = np.array(path)
        ax.plot(path[:, 0], path[:, 1], color='blue', label='Best Path')

        if episode_info:
            episode_num, goal_reached, avg_distance, steps = episode_info
            ax.set_title(f"Episode {episode_num}: Goal Reached {goal_reached}, Avg Distance: {avg_distance:.2f}, Steps: {steps}")
        else:
            ax.set_title(f"Best Path to Goal")

        line, = ax.plot([], [], 'bo-', markersize=5)

        def update(num):
            line.set_data(path[:num, 0], path[:num, 1])
            return line,

        ani = animation.FuncAnimation(fig, update, frames=len(path), interval=pause * 1000, repeat=False)

        # Add a legend and grid
        ax.legend()
        ax.grid(True)

        plt.show()

    def plot_learning_curve(self, rewards_per_episode):
        moving_avg = np.convolve(rewards_per_episode, np.ones(100)/100, mode='valid')

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(moving_avg)
        ax.set_title("Learning Curve (Moving Average of Rewards)")
        ax.set_xlabel("Episodes")
        ax.set_ylabel("Reward")
        plt.show()

    def print_goal_reached_count(self):
        print(f"Total episodes reaching goal: {self.total_reached}")
        print(f"Total episodes failing to reach goal: {self.total_failed}")


# Example usage
env = AircraftEnv()
rewards, steps, distances = env.train(num_episodes=2000, max_steps=1000)

# Plot learning curve and goal count
env.plot_learning_curve(rewards)
env.print_goal_reached_count()

# Show best path animation
env.animate(path=env.best_path, show_details=True, episode_info=env.best_episode_info)
print(f"Best Episode: {env.best_episode_info[0]}, Goal Reached: {env.best_episode_info[1]}, Avg Distance: {env.best_episode_info[2]:.2f}, Steps: {env.best_episode_info[3]}")
